{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sn\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('./Advertising.csv')\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X would represent TV, Radio and Newspaper while Y would represent our sales. As all these sales might be on different scales, we then normalise our X & Y variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df[['TV','radio','newspaper']]\n",
    "Y=df['sales']\n",
    "Y=np.array((Y-Y.mean())/Y.std())\n",
    "X=X.apply(lambda rec:(rec-rec.mean())/rec.std(),axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement a gradient descent algorithm we need to follow 4 steps:  \n",
    "1)Randomly initialize the bias and the weight theta  \n",
    "2)Calculate predicted value of y that is Y given the bias and the weight  \n",
    "3)Calculate the cost function from predicted and actual values of Y  \n",
    "4)Calculate gradient and the weights  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(dim):\n",
    "    b=random.random()\n",
    "    theta=np.random.rand(dim)\n",
    "    return b,theta\n",
    "b,theta=initialize(3)\n",
    "print(\"Bias: \",b,\"Weights: \",theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_Y(b,theta,X):\n",
    "    return b + np.dot(X,theta)\n",
    "Y_hat=predict_Y(b,theta,X)\n",
    "Y_hat[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cost(Y,Y_hat):\n",
    "    Y_resd=Y-Y_hat\n",
    "    return np.sum(np.dot(Y_resd.T,Y_resd))/len(Y-Y_resd)\n",
    "get_cost(Y,Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_theta(x,y,y_hat,b_0,theta_0,learning_rate):\n",
    "    db=(np.sum(y_hat-y)*2)/len(y)\n",
    "    dw=(np.dot((y_hat-y),x)*2)/len(y)\n",
    "    b_1=b_0-learning_rate*db\n",
    "    theta_1=theta_0-learning_rate*dw\n",
    "    return b_1,theta_1\n",
    "print(\"After initialization :- Bias: \",b,\"theta: \",theta)\n",
    "Y_hat=predict_Y(b,theta,X)\n",
    "b,theta=update_theta(X,Y,Y_hat,b,theta,0.001)\n",
    "print(\"After first update :- Bias: \",b,\"theta: \",theta)\n",
    "get_cost(Y,Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X,Y,alpha,num_iterations):\n",
    "    b,theta=initialize(X.shape[1])\n",
    "    iter_num=0\n",
    "    gd_iterations_df=pd.DataFrame(columns=['iteration','cost'])\n",
    "    result_idx=0\n",
    "    for each_iter in range(num_iterations):\n",
    "        Y_hat=predict_Y(b,theta,X)\n",
    "        this_cost=get_cost(Y,Y_hat)\n",
    "        prev_b=b\n",
    "        prev_theta=theta\n",
    "        b,theta=update_theta(X,Y,Y_hat,prev_b,prev_theta,alpha)\n",
    "        if(iter_num%10==0):\n",
    "            gd_iterations_df.loc[result_idx]=[iter_num,this_cost]\n",
    "        result_idx +=1\n",
    "        iter_num +=1\n",
    "    print(\"Final Estimate of b and theta : \",b,theta)\n",
    "    return gd_iterations_df,b,theta\n",
    "gd_iterations_df,b,theta=gradient_descent(X,Y,alpha=0.001,num_iterations=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_iterations_df[0:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore if we print the cost function for each iteration we can see the decrease in the cost function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_values = [0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "num_iterations = 2000\n",
    "for alpha in alpha_values:\n",
    "    alpha_df,b,theta = gradient_descent(X, Y, alpha, num_iterations)\n",
    "    plt.plot(alpha_df['iteration'], alpha_df['cost'], label=f'alpha = {alpha}')\n",
    "plt.xlabel('Number of iterations')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Gradient Descent - Cost vs Iterations')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
